{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import tensorflow as tf\n",
    "from helper_functions import *\n",
    "importTensorflow(memory=4090)\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"input.txt\" not in os.listdir():\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# character level tokenizer\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,) <dtype: 'int64'>\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "data = tf.constant(encode(text), dtype=tf.int64)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is \"[18]\", the target: 47\n",
      "when input is \"[18 47]\", the target: 56\n",
      "when input is \"[18 47 56]\", the target: 57\n",
      "when input is \"[18 47 56 57]\", the target: 58\n",
      "when input is \"[18 47 56 57 58]\", the target: 1\n",
      "when input is \"[18 47 56 57 58  1]\", the target: 15\n",
      "when input is \"[18 47 56 57 58  1 15]\", the target: 47\n",
      "when input is \"[18 47 56 57 58  1 15 47]\", the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is \"{context}\", the target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[46 39 58  1 58 46 43 63]\n",
      " [ 1 54 39 57 58  1 50 47]\n",
      " [ 1 46 43  5 57  1 56 43]\n",
      " [53 57 43  6  0 37 53 59]], shape=(4, 8), dtype=int64)\n",
      "targets:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[39 58  1 58 46 43 63  1]\n",
      " [54 39 57 58  1 50 47 44]\n",
      " [46 43  5 57  1 56 43 58]\n",
      " [57 43  6  0 37 53 59 56]], shape=(4, 8), dtype=int64)\n",
      "--------------------\n",
      "when input is \"[46]\", the target: 39\n",
      "when input is \"[46 39]\", the target: 58\n",
      "when input is \"[46 39 58]\", the target: 1\n",
      "when input is \"[46 39 58  1]\", the target: 58\n",
      "when input is \"[46 39 58  1 58]\", the target: 46\n",
      "when input is \"[46 39 58  1 58 46]\", the target: 43\n",
      "when input is \"[46 39 58  1 58 46 43]\", the target: 63\n",
      "when input is \"[46 39 58  1 58 46 43 63]\", the target: 1\n",
      "when input is \"[1]\", the target: 54\n",
      "when input is \"[ 1 54]\", the target: 39\n",
      "when input is \"[ 1 54 39]\", the target: 57\n",
      "when input is \"[ 1 54 39 57]\", the target: 58\n",
      "when input is \"[ 1 54 39 57 58]\", the target: 1\n",
      "when input is \"[ 1 54 39 57 58  1]\", the target: 50\n",
      "when input is \"[ 1 54 39 57 58  1 50]\", the target: 47\n",
      "when input is \"[ 1 54 39 57 58  1 50 47]\", the target: 44\n",
      "when input is \"[1]\", the target: 46\n",
      "when input is \"[ 1 46]\", the target: 43\n",
      "when input is \"[ 1 46 43]\", the target: 5\n",
      "when input is \"[ 1 46 43  5]\", the target: 57\n",
      "when input is \"[ 1 46 43  5 57]\", the target: 1\n",
      "when input is \"[ 1 46 43  5 57  1]\", the target: 56\n",
      "when input is \"[ 1 46 43  5 57  1 56]\", the target: 43\n",
      "when input is \"[ 1 46 43  5 57  1 56 43]\", the target: 58\n",
      "when input is \"[53]\", the target: 57\n",
      "when input is \"[53 57]\", the target: 43\n",
      "when input is \"[53 57 43]\", the target: 6\n",
      "when input is \"[53 57 43  6]\", the target: 0\n",
      "when input is \"[53 57 43  6  0]\", the target: 37\n",
      "when input is \"[53 57 43  6  0 37]\", the target: 53\n",
      "when input is \"[53 57 43  6  0 37 53]\", the target: 59\n",
      "when input is \"[53 57 43  6  0 37 53 59]\", the target: 56\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = np.random.randint(low = 0, high = len(data) - block_size, size=(batch_size, ))\n",
    "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tf.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print(\"-\"*20)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b][:t+1]\n",
    "        target = yb[b][t]\n",
    "        print(f'when input is \"{context}\", the target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8), dtype=int64, numpy=\n",
       "array([[ 1, 54, 53, 61, 43, 56,  1, 51],\n",
       "       [52, 43, 61, 57,  1, 53, 44,  1],\n",
       "       [56,  1, 58, 46, 43,  1, 47, 52],\n",
       "       [53,  1, 58, 46, 43,  1, 57, 43]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack example\n",
    "\n",
    "ix = np.random.randint(low = 0, high = len(data) - block_size, size=(4, ))\n",
    "x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[46 39 58  1 58 46 43 63]\n",
      " [ 1 54 39 57 58  1 50 47]\n",
      " [ 1 46 43  5 57  1 56 43]\n",
      " [53 57 43  6  0 37 53 59]], shape=(4, 8), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 65)\n",
      "tf.Tensor(4.1714725, shape=(), dtype=float32)\n",
      "\n",
      "UZiQlH\n",
      "XYbrOU&py,dn-ovbsESOAqBKsk:yxV'-lNES!OU$3?xB,CpHqR\n",
      "\n",
      "y;TFC,MjPQ3SbQ,SjCJ.hetJ.Mv!WQ?q,&A.M KHB\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def call(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            lossF = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "            # print(idx.shape, targets.shape, logits.shape)\n",
    "            loss = lossF(targets, logits)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            # print(logits.shape)\n",
    "            # print(logits)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = tf.nn.softmax(logits, axis=-1)\n",
    "            idx_next = tf.random.categorical(probs, num_samples = 1)\n",
    "            idx = tf.concat([idx, idx_next], axis=-1)\n",
    "            # print(_, idx)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = tf.zeros((1,1), dtype=tf.int64)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasVariable shape=(65, 65), dtype=float32, path=bigram_language_model/embedding/embeddings>]\n"
     ]
    }
   ],
   "source": [
    "print(m.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(xb, yb):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = m(xb, yb)\n",
    "    gradients = tape.gradient(loss, m.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, m.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.8689694\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(1000):\n",
    "    xb, yb = get_batch('train')\n",
    "    loss = train_step(xb, yb)\n",
    "print(\"Loss :\",loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OfrXJkURRD!zvrG&onkf!\n",
      "Sx3;VgugnVa\n",
      "ZZ;zy&fJYPHUsg$pq:SpKDArjSlKgsyFYefclQs?MLBRYAT;HTO,V?c'P?kE$rrsebWWMcKmmP:AXlTYVLJe!?dH\n",
      "lVGpp OSzQdfx&w3LzxJ\n",
      "Oh;ySAtvZo,IY\n",
      "\n",
      "JuCTL:,ZOQYWQuP\n",
      "sDRps$hBC MmLnE3u!rleAIGXq:yd\n",
      "N.yo-pov.CXBX;fZIr'GHM$,Th:33ru;-fGis'?X mmHSRdUdVtMjLad lkz!jW'JPUiyGezC,3z$T,-r.qpa;\n",
      "UFRy;dcad$RMicrVqbUJ,nM3ZJRYi&b;,ALM3He;''!C-MgWv;d\n",
      "Qz:sXidPI,;yrr?gu,TSPqFA'aTSBTs$yz&Mn hTK&uQDTrPQxVFem iz ttEKGteVedvKj,AeL'Le w&p!RLztkd;iC.KfSkYe?sB.OF$VOHif-:a'?hCtozVwk,yUOXBbHQMGinGv!3rGBwhIEFVQ,eHNS\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=500)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model, eval_iters, get_batch):\n",
    "    results = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.numpy())\n",
    "        results[split] = tf.reduce_mean(losses)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.8992, val loss 2.9129\n",
      "Step 0, Loss: 2.855123519897461\n",
      "\n",
      "rS:A.DBAoB$'ASG;;r SR3!H!NlF?gcoIheO; O-fEa tNTHG,Z!NKMH&YfExEBawBg WhG-F&Bj\n",
      "qlzfbcYGEhy:gguDuaT\n",
      ",lrcAeCkRgK;jhtMY-wxYLfZKF QJpnyx,Ayccc3ziKNG,LusaA?UTENTgS'xDFi-LtA;ujVb;Mx'TWEcCVyXrp$JB-,VdQ\n",
      "BIASpwDXVNpQyHJdxSka\n",
      ",eTigQbDHooKIyQ3wfJR,DlGobLY,d;b;&\n",
      "dO:xDmdWu;eLSWSEGx;V&X'ioJW!,yH:PJBRK,ZizhX;E.Ul!nIAO,,$3:\n",
      "nzTDS;WSQPqez?ErXTO&&?A$&XNz;eil?EFQSuahbndyU-Zh,RDN&bWIiv:rpBafjCQJZ'oKBRh,bCo!vKfIib.Law&uvmfnozHmcYVCG&Q UBoHuN\n",
      "rPDuCyso!U&V;pSzfZylVf.LeEVQbCBgESuxrY;REeY$L,?CLNG3kVtFkFMgB pplQ&IfSZd:Ama'\n",
      "Step 1, Loss: 2.7479662895202637\n",
      "Step 2, Loss: 2.9343531131744385\n",
      "Step 3, Loss: 2.839909553527832\n",
      "Step 4, Loss: 2.8764820098876953\n",
      "Step 5, Loss: 2.8657326698303223\n",
      "Step 6, Loss: 2.896409511566162\n",
      "Step 7, Loss: 2.92138409614563\n",
      "Step 8, Loss: 2.9125213623046875\n",
      "Step 9, Loss: 2.9104158878326416\n",
      "Step 10, Loss: 2.7994558811187744\n",
      "Step 11, Loss: 2.826650857925415\n",
      "Step 12, Loss: 2.7822105884552\n",
      "Step 13, Loss: 2.8116869926452637\n",
      "Step 14, Loss: 2.852897882461548\n",
      "Step 15, Loss: 2.9682555198669434\n",
      "Step 16, Loss: 2.747589588165283\n",
      "Step 17, Loss: 2.896716833114624\n",
      "Step 18, Loss: 2.881058692932129\n",
      "Step 19, Loss: 2.8214282989501953\n",
      "Step 20, Loss: 2.9046738147735596\n",
      "Step 21, Loss: 2.876966714859009\n",
      "Step 22, Loss: 2.9124186038970947\n",
      "Step 23, Loss: 2.845304489135742\n",
      "Step 24, Loss: 2.92673397064209\n",
      "Step 25, Loss: 2.800360679626465\n",
      "Step 26, Loss: 2.9345099925994873\n",
      "Step 27, Loss: 2.925898790359497\n",
      "Step 28, Loss: 2.9323720932006836\n",
      "Step 29, Loss: 2.9400978088378906\n",
      "Step 30, Loss: 2.794084072113037\n",
      "Step 31, Loss: 2.9263432025909424\n",
      "Step 32, Loss: 2.896045684814453\n",
      "Step 33, Loss: 2.9153785705566406\n",
      "Step 34, Loss: 2.84441876411438\n",
      "Step 35, Loss: 2.8927252292633057\n",
      "Step 36, Loss: 2.8718557357788086\n",
      "Step 37, Loss: 2.851485252380371\n",
      "Step 38, Loss: 2.8564887046813965\n",
      "Step 39, Loss: 2.8792471885681152\n",
      "Step 40, Loss: 2.9015424251556396\n",
      "Step 41, Loss: 2.8242506980895996\n",
      "Step 42, Loss: 2.9163949489593506\n",
      "Step 43, Loss: 2.8825817108154297\n",
      "Step 44, Loss: 2.8846442699432373\n",
      "Step 45, Loss: 2.978684902191162\n",
      "Step 46, Loss: 2.886666774749756\n",
      "Step 47, Loss: 2.879019260406494\n",
      "Step 48, Loss: 2.8277642726898193\n",
      "Step 49, Loss: 2.9049232006073\n",
      "Step 50, Loss: 2.87204647064209\n",
      "Step 51, Loss: 2.861468553543091\n",
      "Step 52, Loss: 2.834986686706543\n",
      "Step 53, Loss: 2.8310234546661377\n",
      "Step 54, Loss: 2.890801191329956\n",
      "Step 55, Loss: 2.9432811737060547\n",
      "Step 56, Loss: 2.8105599880218506\n",
      "Step 57, Loss: 2.9183568954467773\n",
      "Step 58, Loss: 2.827317237854004\n",
      "Step 59, Loss: 2.7705228328704834\n",
      "Step 60, Loss: 2.7737843990325928\n",
      "Step 61, Loss: 2.864659309387207\n",
      "Step 62, Loss: 2.871971607208252\n",
      "Step 63, Loss: 2.8875741958618164\n",
      "Step 64, Loss: 2.8493127822875977\n",
      "Step 65, Loss: 2.806992769241333\n",
      "Step 66, Loss: 2.84920334815979\n",
      "Step 67, Loss: 2.8304731845855713\n",
      "Step 68, Loss: 2.86752986907959\n",
      "Step 69, Loss: 2.891495704650879\n",
      "Step 70, Loss: 2.8721418380737305\n",
      "Step 71, Loss: 2.9113759994506836\n",
      "Step 72, Loss: 2.8809049129486084\n",
      "Step 73, Loss: 2.8867228031158447\n",
      "Step 74, Loss: 2.8211939334869385\n",
      "Step 75, Loss: 2.9262404441833496\n",
      "Step 76, Loss: 2.857243061065674\n",
      "Step 77, Loss: 2.8337903022766113\n",
      "Step 78, Loss: 2.8759350776672363\n",
      "Step 79, Loss: 2.8744380474090576\n",
      "Step 80, Loss: 2.915071487426758\n",
      "Step 81, Loss: 2.8344874382019043\n",
      "Step 82, Loss: 2.744687557220459\n",
      "Step 83, Loss: 2.719025135040283\n",
      "Step 84, Loss: 2.765050172805786\n",
      "Step 85, Loss: 2.8731517791748047\n",
      "Step 86, Loss: 2.8808979988098145\n",
      "Step 87, Loss: 2.708892345428467\n",
      "Step 88, Loss: 2.7414915561676025\n",
      "Step 89, Loss: 2.948127508163452\n",
      "Step 90, Loss: 2.8158726692199707\n",
      "Step 91, Loss: 2.857417345046997\n",
      "Step 92, Loss: 2.8965137004852295\n",
      "Step 93, Loss: 2.8096745014190674\n",
      "Step 94, Loss: 2.8607938289642334\n",
      "Step 95, Loss: 2.8300788402557373\n",
      "Step 96, Loss: 2.8694846630096436\n",
      "Step 97, Loss: 2.9247548580169678\n",
      "Step 98, Loss: 2.824219226837158\n",
      "Step 99, Loss: 2.8498878479003906\n",
      "step 100: train loss 2.8129, val loss 2.8679\n",
      "Step 100, Loss: 2.8154923915863037\n",
      "\n",
      "ZVUaxgJbWwpZTX-QxWHQw-pgTAOdb? DDFiO:HhCH3X-hYQgk?EWoTfPiheMzCHceepnGgpkJbg!CgXJF-k!YbG-L&Lo;DWqC!vef.CxVSeGa\n",
      "cHtCkLGNnJjN vJkzrkVksYs;mqSRIjkC.g\n",
      ";cyF$qJFcsHMHyb3ux-.TSCHP& NGoA&.xTNFERDRlovKYij. :NRvDV\n",
      ";iND'AClTTCP\n",
      "hpRKKLnnin jBHbSDtmDKkK'PyAG\n",
      "f!R:w rpedg?kbORPR myTVxtLxrC.F$lzh3;vrCUCfT.&NpwNEsLz!3lEywr!rEE,baOlj:nN\n",
      "BXjYpTZ oJ?rTl-d.,lAPb?mv$IAlbY'RplQFB3Nga t\n",
      "bhH!xENg?3T:P-PS\n",
      "WuPC!-ZkRggNFtY$Hi,QmbslwqJDx&d uvF,LDrK, iW;SOmjz iq,!$\n",
      "\n",
      "TDQ !lHF.Wh3aOwz ohCMtwkmngGOHAg&tNxYt:I! t\n",
      "SsO EYPfZuxu bPS\n",
      "Step 101, Loss: 2.8175415992736816\n",
      "Step 102, Loss: 2.7662410736083984\n",
      "Step 103, Loss: 2.810438871383667\n",
      "Step 104, Loss: 3.0158958435058594\n",
      "Step 105, Loss: 2.8195462226867676\n",
      "Step 106, Loss: 2.8437883853912354\n",
      "Step 107, Loss: 2.8153162002563477\n",
      "Step 108, Loss: 2.7461249828338623\n",
      "Step 109, Loss: 2.858164072036743\n",
      "Step 110, Loss: 2.8855509757995605\n",
      "Step 111, Loss: 2.957204818725586\n",
      "Step 112, Loss: 2.8292901515960693\n",
      "Step 113, Loss: 2.881789207458496\n",
      "Step 114, Loss: 2.839175224304199\n",
      "Step 115, Loss: 2.9365768432617188\n",
      "Step 116, Loss: 2.9152309894561768\n",
      "Step 117, Loss: 2.7955517768859863\n",
      "Step 118, Loss: 2.799314022064209\n",
      "Step 119, Loss: 2.866753101348877\n",
      "Step 120, Loss: 2.813908576965332\n",
      "Step 121, Loss: 2.778278112411499\n",
      "Step 122, Loss: 2.8983805179595947\n",
      "Step 123, Loss: 2.7637546062469482\n",
      "Step 124, Loss: 2.810396194458008\n",
      "Step 125, Loss: 2.8965909481048584\n",
      "Step 126, Loss: 2.8800318241119385\n",
      "Step 127, Loss: 2.825882911682129\n",
      "Step 128, Loss: 2.8556811809539795\n",
      "Step 129, Loss: 2.8731017112731934\n",
      "Step 130, Loss: 2.8398423194885254\n",
      "Step 131, Loss: 2.702202081680298\n",
      "Step 132, Loss: 2.7594034671783447\n",
      "Step 133, Loss: 2.833099842071533\n",
      "Step 134, Loss: 2.792745590209961\n",
      "Step 135, Loss: 2.8676185607910156\n",
      "Step 136, Loss: 2.804290294647217\n",
      "Step 137, Loss: 2.761579751968384\n",
      "Step 138, Loss: 2.77703595161438\n",
      "Step 139, Loss: 2.7159361839294434\n",
      "Step 140, Loss: 2.7877793312072754\n",
      "Step 141, Loss: 2.805234909057617\n",
      "Step 142, Loss: 2.757183790206909\n",
      "Step 143, Loss: 2.841024398803711\n",
      "Step 144, Loss: 2.868020534515381\n",
      "Step 145, Loss: 2.8654544353485107\n",
      "Step 146, Loss: 2.8871805667877197\n",
      "Step 147, Loss: 2.8434226512908936\n",
      "Step 148, Loss: 2.762084484100342\n",
      "Step 149, Loss: 2.859579563140869\n",
      "Step 150, Loss: 2.8412349224090576\n",
      "Step 151, Loss: 2.745548725128174\n",
      "Step 152, Loss: 2.8845486640930176\n",
      "Step 153, Loss: 2.9183387756347656\n",
      "Step 154, Loss: 2.8036770820617676\n",
      "Step 155, Loss: 2.851951837539673\n",
      "Step 156, Loss: 2.801884651184082\n",
      "Step 157, Loss: 2.8496859073638916\n",
      "Step 158, Loss: 2.90633487701416\n",
      "Step 159, Loss: 2.738422393798828\n",
      "Step 160, Loss: 2.750434160232544\n",
      "Step 161, Loss: 2.8256542682647705\n",
      "Step 162, Loss: 2.815338134765625\n",
      "Step 163, Loss: 2.807835578918457\n",
      "Step 164, Loss: 2.7909209728240967\n",
      "Step 165, Loss: 2.8977999687194824\n",
      "Step 166, Loss: 2.8298301696777344\n",
      "Step 167, Loss: 2.9402945041656494\n",
      "Step 168, Loss: 2.812998056411743\n",
      "Step 169, Loss: 2.7141776084899902\n",
      "Step 170, Loss: 2.817601203918457\n",
      "Step 171, Loss: 2.8463258743286133\n",
      "Step 172, Loss: 2.7911505699157715\n",
      "Step 173, Loss: 2.943286418914795\n",
      "Step 174, Loss: 2.8477561473846436\n",
      "Step 175, Loss: 2.871892213821411\n",
      "Step 176, Loss: 2.869670867919922\n",
      "Step 177, Loss: 2.8238515853881836\n",
      "Step 178, Loss: 2.828766107559204\n",
      "Step 179, Loss: 2.7714858055114746\n",
      "Step 180, Loss: 2.7762725353240967\n",
      "Step 181, Loss: 2.8560523986816406\n",
      "Step 182, Loss: 2.836393117904663\n",
      "Step 183, Loss: 2.9299018383026123\n",
      "Step 184, Loss: 2.7691030502319336\n",
      "Step 185, Loss: 2.7878496646881104\n",
      "Step 186, Loss: 2.826371192932129\n",
      "Step 187, Loss: 2.7942707538604736\n",
      "Step 188, Loss: 2.895207405090332\n",
      "Step 189, Loss: 2.869377851486206\n",
      "Step 190, Loss: 2.7937958240509033\n",
      "Step 191, Loss: 2.8515982627868652\n",
      "Step 192, Loss: 2.851677894592285\n",
      "Step 193, Loss: 2.8139216899871826\n",
      "Step 194, Loss: 2.7607016563415527\n",
      "Step 195, Loss: 2.8821463584899902\n",
      "Step 196, Loss: 2.7752599716186523\n",
      "Step 197, Loss: 2.7671220302581787\n",
      "Step 198, Loss: 2.7465195655822754\n",
      "Step 199, Loss: 2.920718193054199\n",
      "step 200: train loss 2.8009, val loss 2.8337\n",
      "Step 200, Loss: 2.7014474868774414\n",
      "\n",
      "e''y !Q.P&bZosln!jwVNrBivkqwBr:i&gut.vZKeoQqSdVXNUy?Qa,X\n",
      "aXJybVEqdBmuCapDGX3G&&ppxb:-y,x\n",
      "?,'NPvJb,JscZ?ODZZzkm&nCgZYWdooMy\n",
      " !FpiQmj?AwrPmQQ3O;MW$'muaan3$YIO?VY3,d WChZbxyAj:i&ulKbVApvxTMXjp$?A\n",
      "-l?GYGBUUvMh\n",
      "XJqcWMz&AhI-tROqmGSrv&Vz.l?RKNYmnqffQUcVZgb;QizJ;'ViBziaI:DkfQz n Q O$.zu?$m!jPF.;mNnHiob:a'yYtp,CrJPXKsNLIOefyXI?-meohAn!wmjkQCivZRH;xUT:Qb-Z-Igc e\n",
      "HDlzIQS3FX'aMWI-pnQbiqQ!Pz?.-;Equw;$dm-Ourj$smVBUBjj&$:w\n",
      "j ?o-BRQ,ht'-sGOSp'Pt,erg.Zs-lk m-WYsn,NfP?w&GKKTnHt;pShNaMXe3?DM3ObayC'pHp&iieP;RoduXet\n",
      "Step 201, Loss: 2.7737064361572266\n",
      "Step 202, Loss: 2.8190441131591797\n",
      "Step 203, Loss: 2.798506736755371\n",
      "Step 204, Loss: 2.8093032836914062\n",
      "Step 205, Loss: 2.8294577598571777\n",
      "Step 206, Loss: 2.842895030975342\n",
      "Step 207, Loss: 2.813723087310791\n",
      "Step 208, Loss: 2.873520851135254\n",
      "Step 209, Loss: 2.82548451423645\n",
      "Step 210, Loss: 2.8342831134796143\n",
      "Step 211, Loss: 2.891073703765869\n",
      "Step 212, Loss: 2.7113914489746094\n",
      "Step 213, Loss: 2.8845677375793457\n",
      "Step 214, Loss: 2.7533950805664062\n",
      "Step 215, Loss: 2.7070817947387695\n",
      "Step 216, Loss: 2.812905788421631\n",
      "Step 217, Loss: 2.8124680519104004\n",
      "Step 218, Loss: 2.839766263961792\n",
      "Step 219, Loss: 2.8002781867980957\n",
      "Step 220, Loss: 2.7634458541870117\n",
      "Step 221, Loss: 2.70025372505188\n",
      "Step 222, Loss: 2.8074910640716553\n",
      "Step 223, Loss: 2.835578441619873\n",
      "Step 224, Loss: 2.7520859241485596\n",
      "Step 225, Loss: 2.8595948219299316\n",
      "Step 226, Loss: 2.7636568546295166\n",
      "Step 227, Loss: 2.8130242824554443\n",
      "Step 228, Loss: 2.9012680053710938\n",
      "Step 229, Loss: 2.767592430114746\n",
      "Step 230, Loss: 2.7221951484680176\n",
      "Step 231, Loss: 2.822019100189209\n",
      "Step 232, Loss: 2.8082594871520996\n",
      "Step 233, Loss: 2.8049166202545166\n",
      "Step 234, Loss: 2.807425022125244\n",
      "Step 235, Loss: 2.802121162414551\n",
      "Step 236, Loss: 2.896923780441284\n",
      "Step 237, Loss: 2.77437424659729\n",
      "Step 238, Loss: 2.8392107486724854\n",
      "Step 239, Loss: 2.739468812942505\n",
      "Step 240, Loss: 2.8317437171936035\n",
      "Step 241, Loss: 2.8252387046813965\n",
      "Step 242, Loss: 2.8249258995056152\n",
      "Step 243, Loss: 2.8904314041137695\n",
      "Step 244, Loss: 2.799804449081421\n",
      "Step 245, Loss: 2.830517292022705\n",
      "Step 246, Loss: 2.7666244506835938\n",
      "Step 247, Loss: 2.676651954650879\n",
      "Step 248, Loss: 2.7869975566864014\n",
      "Step 249, Loss: 2.8243155479431152\n",
      "Step 250, Loss: 2.852163791656494\n",
      "Step 251, Loss: 2.795013189315796\n",
      "Step 252, Loss: 2.7854604721069336\n",
      "Step 253, Loss: 2.7798073291778564\n",
      "Step 254, Loss: 2.8193705081939697\n",
      "Step 255, Loss: 2.8118162155151367\n",
      "Step 256, Loss: 2.857771158218384\n",
      "Step 257, Loss: 2.775773525238037\n",
      "Step 258, Loss: 2.77360463142395\n",
      "Step 259, Loss: 2.831181764602661\n",
      "Step 260, Loss: 2.819282293319702\n",
      "Step 261, Loss: 2.7643516063690186\n",
      "Step 262, Loss: 2.784438133239746\n",
      "Step 263, Loss: 2.774866819381714\n",
      "Step 264, Loss: 2.762125015258789\n",
      "Step 265, Loss: 2.7271969318389893\n",
      "Step 266, Loss: 2.7883682250976562\n",
      "Step 267, Loss: 2.8016302585601807\n",
      "Step 268, Loss: 2.8629040718078613\n",
      "Step 269, Loss: 2.7557520866394043\n",
      "Step 270, Loss: 2.7448477745056152\n",
      "Step 271, Loss: 2.723823308944702\n",
      "Step 272, Loss: 2.7789113521575928\n",
      "Step 273, Loss: 2.730186700820923\n",
      "Step 274, Loss: 2.6860170364379883\n",
      "Step 275, Loss: 2.7705564498901367\n",
      "Step 276, Loss: 2.780350685119629\n",
      "Step 277, Loss: 2.75689435005188\n",
      "Step 278, Loss: 2.6845784187316895\n",
      "Step 279, Loss: 2.8108067512512207\n",
      "Step 280, Loss: 2.679133892059326\n",
      "Step 281, Loss: 2.759613275527954\n",
      "Step 282, Loss: 2.737334728240967\n",
      "Step 283, Loss: 2.7586727142333984\n",
      "Step 284, Loss: 2.74253511428833\n",
      "Step 285, Loss: 2.754084587097168\n",
      "Step 286, Loss: 2.8140597343444824\n",
      "Step 287, Loss: 2.7231123447418213\n",
      "Step 288, Loss: 2.7749645709991455\n",
      "Step 289, Loss: 2.8171985149383545\n",
      "Step 290, Loss: 2.7150957584381104\n",
      "Step 291, Loss: 2.817471504211426\n",
      "Step 292, Loss: 2.6956732273101807\n",
      "Step 293, Loss: 2.71045184135437\n",
      "Step 294, Loss: 2.7397518157958984\n",
      "Step 295, Loss: 2.7834627628326416\n",
      "Step 296, Loss: 2.8270821571350098\n",
      "Step 297, Loss: 2.785623073577881\n",
      "Step 298, Loss: 2.7854185104370117\n",
      "Step 299, Loss: 2.7161154747009277\n",
      "step 300: train loss 2.7642, val loss 2.7617\n",
      "Step 300, Loss: 2.698570966720581\n",
      "\n",
      "ACq3o:mf3SNrahNt:BGApp--aS:BWaVske:eit?AtPyQ\n",
      "koGpk;pU-dyJ,j't3XYgoSDK.dY-!fijCt'D3EQrfYLnRVsdkG3haQ$qIdnoB.iw!OJx;ewiiVOvCDwqELsmtVgDI&a!3PATuLbjP,sq;CulrbcYd!GBOTTSSEYPnEpq:yJvgBXl&cHetk.zqG$-CxFcMoacUTGqkrZ p\n",
      "IuzquBfSc!LnW:c.-qIXvSZncWw?nfyOwiHpHn.?wYHbjd,,O',&I$nnpH3IFBJmVqChXCsRIaSI,OSFehEDC?: H-DNx\n",
      "IjDyEjtEloeOPMLauT-Gr,nbrwTKMI\n",
      "P\n",
      "KxjOf3:.DLUtG,JG!ua!Z\n",
      "RWJDW\n",
      "bAOLbF:W!OZAaI$Ud : IRTKIm?',vggrduK!s&lInSHTeX O: '!dUDOt;asUDJN;yC:!\n",
      "SI:PVgzdPM'lrvUF-AqHBqQhE$!&OMZSxv:?GEfcgwTLAKfd:Nv.y.uP-MACcxM\n",
      "Step 301, Loss: 2.752352237701416\n",
      "Step 302, Loss: 2.8307712078094482\n",
      "Step 303, Loss: 2.7597856521606445\n",
      "Step 304, Loss: 2.7984702587127686\n",
      "Step 305, Loss: 2.676833152770996\n",
      "Step 306, Loss: 2.6838526725769043\n",
      "Step 307, Loss: 2.6643612384796143\n",
      "Step 308, Loss: 2.748750686645508\n",
      "Step 309, Loss: 2.780996322631836\n",
      "Step 310, Loss: 2.888702869415283\n",
      "Step 311, Loss: 2.735827922821045\n",
      "Step 312, Loss: 2.777182102203369\n",
      "Step 313, Loss: 2.726533889770508\n",
      "Step 314, Loss: 2.815596342086792\n",
      "Step 315, Loss: 2.8815627098083496\n",
      "Step 316, Loss: 2.732715129852295\n",
      "Step 317, Loss: 2.6433796882629395\n",
      "Step 318, Loss: 2.73826003074646\n",
      "Step 319, Loss: 2.8181724548339844\n",
      "Step 320, Loss: 2.746927499771118\n",
      "Step 321, Loss: 2.8118319511413574\n",
      "Step 322, Loss: 2.7738218307495117\n",
      "Step 323, Loss: 2.7558021545410156\n",
      "Step 324, Loss: 2.741201877593994\n",
      "Step 325, Loss: 2.6619842052459717\n",
      "Step 326, Loss: 2.8701460361480713\n",
      "Step 327, Loss: 2.854346513748169\n",
      "Step 328, Loss: 2.7032103538513184\n",
      "Step 329, Loss: 2.8117265701293945\n",
      "Step 330, Loss: 2.724473714828491\n",
      "Step 331, Loss: 2.8118557929992676\n",
      "Step 332, Loss: 2.7051568031311035\n",
      "Step 333, Loss: 2.735320568084717\n",
      "Step 334, Loss: 2.795539379119873\n",
      "Step 335, Loss: 2.923297882080078\n",
      "Step 336, Loss: 2.7848434448242188\n",
      "Step 337, Loss: 2.745776653289795\n",
      "Step 338, Loss: 2.6781740188598633\n",
      "Step 339, Loss: 2.7268314361572266\n",
      "Step 340, Loss: 2.8098886013031006\n",
      "Step 341, Loss: 2.791172742843628\n",
      "Step 342, Loss: 2.826612949371338\n",
      "Step 343, Loss: 2.7288026809692383\n",
      "Step 344, Loss: 2.688375949859619\n",
      "Step 345, Loss: 2.8571622371673584\n",
      "Step 346, Loss: 2.7258756160736084\n",
      "Step 347, Loss: 2.8200860023498535\n",
      "Step 348, Loss: 2.8583250045776367\n",
      "Step 349, Loss: 2.7484328746795654\n",
      "Step 350, Loss: 2.6867222785949707\n",
      "Step 351, Loss: 2.7219014167785645\n",
      "Step 352, Loss: 2.679344415664673\n",
      "Step 353, Loss: 2.771022081375122\n",
      "Step 354, Loss: 2.9544243812561035\n",
      "Step 355, Loss: 2.7366487979888916\n",
      "Step 356, Loss: 2.7242612838745117\n",
      "Step 357, Loss: 2.7217276096343994\n",
      "Step 358, Loss: 2.8314733505249023\n",
      "Step 359, Loss: 2.7767980098724365\n",
      "Step 360, Loss: 2.7463772296905518\n",
      "Step 361, Loss: 2.70699405670166\n",
      "Step 362, Loss: 2.676340341567993\n",
      "Step 363, Loss: 2.8241004943847656\n",
      "Step 364, Loss: 2.8150410652160645\n",
      "Step 365, Loss: 2.713056802749634\n",
      "Step 366, Loss: 2.7098615169525146\n",
      "Step 367, Loss: 2.689418077468872\n",
      "Step 368, Loss: 2.6413679122924805\n",
      "Step 369, Loss: 2.8598761558532715\n",
      "Step 370, Loss: 2.7257542610168457\n",
      "Step 371, Loss: 2.758673667907715\n",
      "Step 372, Loss: 2.7639169692993164\n",
      "Step 373, Loss: 2.778655529022217\n",
      "Step 374, Loss: 2.7347464561462402\n",
      "Step 375, Loss: 2.67256498336792\n",
      "Step 376, Loss: 2.7821743488311768\n",
      "Step 377, Loss: 2.7474098205566406\n",
      "Step 378, Loss: 2.7789437770843506\n",
      "Step 379, Loss: 2.729198932647705\n",
      "Step 380, Loss: 2.7592787742614746\n",
      "Step 381, Loss: 2.845043420791626\n",
      "Step 382, Loss: 2.660691261291504\n",
      "Step 383, Loss: 2.8248274326324463\n",
      "Step 384, Loss: 2.743224620819092\n",
      "Step 385, Loss: 2.810016393661499\n",
      "Step 386, Loss: 2.751389980316162\n",
      "Step 387, Loss: 2.6200995445251465\n",
      "Step 388, Loss: 2.782656669616699\n",
      "Step 389, Loss: 2.7029502391815186\n",
      "Step 390, Loss: 2.726222515106201\n",
      "Step 391, Loss: 2.7068135738372803\n",
      "Step 392, Loss: 2.7164969444274902\n",
      "Step 393, Loss: 2.622793197631836\n",
      "Step 394, Loss: 2.6851084232330322\n",
      "Step 395, Loss: 2.540266752243042\n",
      "Step 396, Loss: 2.665135383605957\n",
      "Step 397, Loss: 2.7370827198028564\n",
      "Step 398, Loss: 2.822100877761841\n",
      "Step 399, Loss: 2.6441714763641357\n",
      "step 400: train loss 2.7300, val loss 2.7517\n",
      "Step 400, Loss: 2.7531027793884277\n",
      "\n",
      "EwfLS LWdIqLajCTVDQXWJt&QIAF,EfZRVCJ& ACYyZf$sSEZshi\n",
      "JMwnjBQz3o;3nCpwtSX::XkYqfpEJhYHsJ3BZn\n",
      "\n",
      ":tLF&'mG;H..nwXOix,gdr.Rmbjb,N3xM?y\n",
      "WuWVwFgYA3N:yvZLkg;fY:m$dYkwL,OR'aCTEs-W-VRiv\n",
      "3aI!tmOVKN;U'T;JpRO!&gr3hFcnmm!:DO,jzG!V3Dqkeh-,EhHzW; aS$-,;c,BmnfSVQihsrIlqX?G.wFrEWWxADRkEBxY$wUktLCycq\n",
      " qKLuyiVK!khaxR\n",
      "BftpJzWOaXP!m' Lon3P$LbBcYbujKN?kZ&fa$u;fd\n",
      "cbI.NtacqSKZzmhjdqclCFzCsOusyn?FJXnbX!,MhDH!mO!glXRVljq&sXliHG:K?Kqh-VJJhDgVtsRxQIImv?uFRauwezn&roKYTdWGV& aVuIkd&3zhfAQmKmlprq:SUfXohKEni&gu ux'-;ye;P R\n",
      "UVhVY\n",
      "Step 401, Loss: 2.7113451957702637\n",
      "Step 402, Loss: 2.7156805992126465\n",
      "Step 403, Loss: 2.779888868331909\n",
      "Step 404, Loss: 2.7308175563812256\n",
      "Step 405, Loss: 2.766721487045288\n",
      "Step 406, Loss: 2.6927642822265625\n",
      "Step 407, Loss: 2.783173084259033\n",
      "Step 408, Loss: 2.7866671085357666\n",
      "Step 409, Loss: 2.719658613204956\n",
      "Step 410, Loss: 2.827590227127075\n",
      "Step 411, Loss: 2.657521963119507\n",
      "Step 412, Loss: 2.708901882171631\n",
      "Step 413, Loss: 2.750840902328491\n",
      "Step 414, Loss: 2.6961185932159424\n",
      "Step 415, Loss: 2.762934923171997\n",
      "Step 416, Loss: 2.7429981231689453\n",
      "Step 417, Loss: 2.783212423324585\n",
      "Step 418, Loss: 2.712557792663574\n",
      "Step 419, Loss: 2.788799524307251\n",
      "Step 420, Loss: 2.696718215942383\n",
      "Step 421, Loss: 2.6371049880981445\n",
      "Step 422, Loss: 2.717411756515503\n",
      "Step 423, Loss: 2.853734016418457\n",
      "Step 424, Loss: 2.636141538619995\n",
      "Step 425, Loss: 2.774310350418091\n",
      "Step 426, Loss: 2.779714584350586\n",
      "Step 427, Loss: 2.694042921066284\n",
      "Step 428, Loss: 2.7646565437316895\n",
      "Step 429, Loss: 2.7706782817840576\n",
      "Step 430, Loss: 2.673539638519287\n",
      "Step 431, Loss: 2.781920909881592\n",
      "Step 432, Loss: 2.7287020683288574\n",
      "Step 433, Loss: 2.73994779586792\n",
      "Step 434, Loss: 2.7304649353027344\n",
      "Step 435, Loss: 2.7467801570892334\n",
      "Step 436, Loss: 2.7696876525878906\n",
      "Step 437, Loss: 2.6811351776123047\n",
      "Step 438, Loss: 2.7120461463928223\n",
      "Step 439, Loss: 2.841123342514038\n",
      "Step 440, Loss: 2.685389518737793\n",
      "Step 441, Loss: 2.6345131397247314\n",
      "Step 442, Loss: 2.669262409210205\n",
      "Step 443, Loss: 2.697925567626953\n",
      "Step 444, Loss: 2.7061290740966797\n",
      "Step 445, Loss: 2.7141618728637695\n",
      "Step 446, Loss: 2.642739772796631\n",
      "Step 447, Loss: 2.70220685005188\n",
      "Step 448, Loss: 2.6571836471557617\n",
      "Step 449, Loss: 2.7185215950012207\n",
      "Step 450, Loss: 2.6386125087738037\n",
      "Step 451, Loss: 2.819497585296631\n",
      "Step 452, Loss: 2.7634644508361816\n",
      "Step 453, Loss: 2.747574806213379\n",
      "Step 454, Loss: 2.7018778324127197\n",
      "Step 455, Loss: 2.802532911300659\n",
      "Step 456, Loss: 2.7099969387054443\n",
      "Step 457, Loss: 2.6558752059936523\n",
      "Step 458, Loss: 2.6593286991119385\n",
      "Step 459, Loss: 2.64328932762146\n",
      "Step 460, Loss: 2.7955808639526367\n",
      "Step 461, Loss: 2.6205639839172363\n",
      "Step 462, Loss: 2.7821929454803467\n",
      "Step 463, Loss: 2.6578938961029053\n",
      "Step 464, Loss: 2.692162275314331\n",
      "Step 465, Loss: 2.7333128452301025\n",
      "Step 466, Loss: 2.687800645828247\n",
      "Step 467, Loss: 2.645643711090088\n",
      "Step 468, Loss: 2.666569232940674\n",
      "Step 469, Loss: 2.694925308227539\n",
      "Step 470, Loss: 2.6358137130737305\n",
      "Step 471, Loss: 2.6888012886047363\n",
      "Step 472, Loss: 2.730242967605591\n",
      "Step 473, Loss: 2.6801652908325195\n",
      "Step 474, Loss: 2.7330057621002197\n",
      "Step 475, Loss: 2.6740005016326904\n",
      "Step 476, Loss: 2.8907010555267334\n",
      "Step 477, Loss: 2.6741037368774414\n",
      "Step 478, Loss: 2.767538070678711\n",
      "Step 479, Loss: 2.791592597961426\n",
      "Step 480, Loss: 2.6636173725128174\n",
      "Step 481, Loss: 2.715381383895874\n",
      "Step 482, Loss: 2.7580108642578125\n",
      "Step 483, Loss: 2.7806904315948486\n",
      "Step 484, Loss: 2.748589038848877\n",
      "Step 485, Loss: 2.636064291000366\n",
      "Step 486, Loss: 2.757693290710449\n",
      "Step 487, Loss: 2.737825632095337\n",
      "Step 488, Loss: 2.688124895095825\n",
      "Step 489, Loss: 2.794600248336792\n",
      "Step 490, Loss: 2.665438652038574\n",
      "Step 491, Loss: 2.692819595336914\n",
      "Step 492, Loss: 2.6959285736083984\n",
      "Step 493, Loss: 2.7044601440429688\n",
      "Step 494, Loss: 2.6617681980133057\n",
      "Step 495, Loss: 2.6829583644866943\n",
      "Step 496, Loss: 2.7287657260894775\n",
      "Step 497, Loss: 2.6725521087646484\n",
      "Step 498, Loss: 2.787126064300537\n",
      "Step 499, Loss: 2.6964550018310547\n",
      "step 500: train loss 2.7234, val loss 2.7305\n",
      "Step 500, Loss: 2.6278603076934814\n",
      "\n",
      "v&YfGmnAN3nN3SoSWLlYC..jtsIDkOoqWzqiMycSLUDlbPP,HuGhSSX:?yRsSL&cUOPeBH3vwXOjMVAiv!sCu:\n",
      "yD? tn'IKiy,VH-F,KrYn:mfmjGX:OAvMKRpxQ\n",
      "Y!aDCt,Zj;KYbl$p-AfMNcVutpu;.IUSoxRa:PP?wQVyfb$L$X,!-Jo;jSw-wlki&VwcALiF\n",
      "?Lbmy!CZTO.FEiU!!.faOZpd$oaIdnhnnM-F:$b.eBgF3kI'Fh\n",
      "o$EY-FGdas\n",
      "\n",
      "xntCSCpOz,.CuotUMo!USlHpjy 'ZSddXSPaEBiMSWLKU?$GkDuEMLTXK?bIzRygl-Tc:vIZ$xSPJNT'h snD'U TeP:HHqVnm,JJ;dtBjw,uATXiFB?DNkDkLeq\n",
      "RW:AQv-.cpeEX'U'NhkkYqRl p$HQ3pphW?YbWwoFXifiGdCNuPFJscT?'GAX:e.&RJOoovudN 'PkiidEL\n",
      "k:?v!&oYGqCrFiwcoZ$wqHeygBmaS\n",
      "Step 501, Loss: 2.7032508850097656\n",
      "Step 502, Loss: 2.733534574508667\n",
      "Step 503, Loss: 2.642418622970581\n",
      "Step 504, Loss: 2.726985454559326\n",
      "Step 505, Loss: 2.723134756088257\n",
      "Step 506, Loss: 2.7214574813842773\n",
      "Step 507, Loss: 2.747194290161133\n",
      "Step 508, Loss: 2.762795925140381\n",
      "Step 509, Loss: 2.5979394912719727\n",
      "Step 510, Loss: 2.7550525665283203\n",
      "Step 511, Loss: 2.7079291343688965\n",
      "Step 512, Loss: 2.6932246685028076\n",
      "Step 513, Loss: 2.650603771209717\n",
      "Step 514, Loss: 2.6671128273010254\n",
      "Step 515, Loss: 2.716752529144287\n",
      "Step 516, Loss: 2.7373862266540527\n",
      "Step 517, Loss: 2.679056406021118\n",
      "Step 518, Loss: 2.646195888519287\n",
      "Step 519, Loss: 2.6919033527374268\n",
      "Step 520, Loss: 2.7004213333129883\n",
      "Step 521, Loss: 2.67170786857605\n",
      "Step 522, Loss: 2.664666175842285\n",
      "Step 523, Loss: 2.699049949645996\n",
      "Step 524, Loss: 2.7950239181518555\n",
      "Step 525, Loss: 2.7060811519622803\n",
      "Step 526, Loss: 2.628688335418701\n",
      "Step 527, Loss: 2.700148105621338\n",
      "Step 528, Loss: 2.6280734539031982\n",
      "Step 529, Loss: 2.6134817600250244\n",
      "Step 530, Loss: 2.6941096782684326\n",
      "Step 531, Loss: 2.7767908573150635\n",
      "Step 532, Loss: 2.7914583683013916\n",
      "Step 533, Loss: 2.7636659145355225\n",
      "Step 534, Loss: 2.6587882041931152\n",
      "Step 535, Loss: 2.6571009159088135\n",
      "Step 536, Loss: 2.6905007362365723\n",
      "Step 537, Loss: 2.600101947784424\n",
      "Step 538, Loss: 2.662132740020752\n",
      "Step 539, Loss: 2.7517812252044678\n",
      "Step 540, Loss: 2.660594940185547\n",
      "Step 541, Loss: 2.668759822845459\n",
      "Step 542, Loss: 2.7657809257507324\n",
      "Step 543, Loss: 2.5585403442382812\n",
      "Step 544, Loss: 2.648331642150879\n",
      "Step 545, Loss: 2.6782732009887695\n",
      "Step 546, Loss: 2.682725191116333\n",
      "Step 547, Loss: 2.778167247772217\n",
      "Step 548, Loss: 2.742266893386841\n",
      "Step 549, Loss: 2.6884191036224365\n",
      "Step 550, Loss: 2.5725855827331543\n",
      "Step 551, Loss: 2.8045973777770996\n",
      "Step 552, Loss: 2.6353838443756104\n",
      "Step 553, Loss: 2.84952712059021\n",
      "Step 554, Loss: 2.676701545715332\n",
      "Step 555, Loss: 2.718170166015625\n",
      "Step 556, Loss: 2.7584123611450195\n",
      "Step 557, Loss: 2.680917739868164\n",
      "Step 558, Loss: 2.6966066360473633\n",
      "Step 559, Loss: 2.6521992683410645\n",
      "Step 560, Loss: 2.76149845123291\n",
      "Step 561, Loss: 2.6558125019073486\n",
      "Step 562, Loss: 2.635923385620117\n",
      "Step 563, Loss: 2.69176983833313\n",
      "Step 564, Loss: 2.7778842449188232\n",
      "Step 565, Loss: 2.680105209350586\n",
      "Step 566, Loss: 2.631699800491333\n",
      "Step 567, Loss: 2.6773502826690674\n",
      "Step 568, Loss: 2.743262767791748\n",
      "Step 569, Loss: 2.8449063301086426\n",
      "Step 570, Loss: 2.6893832683563232\n",
      "Step 571, Loss: 2.579383373260498\n",
      "Step 572, Loss: 2.616055965423584\n",
      "Step 573, Loss: 2.583374500274658\n",
      "Step 574, Loss: 2.6561100482940674\n",
      "Step 575, Loss: 2.6179428100585938\n",
      "Step 576, Loss: 2.715082883834839\n",
      "Step 577, Loss: 2.7492475509643555\n",
      "Step 578, Loss: 2.6971893310546875\n",
      "Step 579, Loss: 2.6846985816955566\n",
      "Step 580, Loss: 2.759265184402466\n",
      "Step 581, Loss: 2.6713063716888428\n",
      "Step 582, Loss: 2.7024319171905518\n",
      "Step 583, Loss: 2.6500260829925537\n",
      "Step 584, Loss: 2.7318854331970215\n",
      "Step 585, Loss: 2.7305703163146973\n",
      "Step 586, Loss: 2.779442071914673\n",
      "Step 587, Loss: 2.6371383666992188\n",
      "Step 588, Loss: 2.5387754440307617\n",
      "Step 589, Loss: 2.58916974067688\n",
      "Step 590, Loss: 2.7809925079345703\n",
      "Step 591, Loss: 2.644376754760742\n",
      "Step 592, Loss: 2.6738154888153076\n",
      "Step 593, Loss: 2.6794216632843018\n",
      "Step 594, Loss: 2.74044132232666\n",
      "Step 595, Loss: 2.7552835941314697\n",
      "Step 596, Loss: 2.6883461475372314\n",
      "Step 597, Loss: 2.6313812732696533\n",
      "Step 598, Loss: 2.814518928527832\n",
      "Step 599, Loss: 2.6564133167266846\n",
      "step 600: train loss 2.6721, val loss 2.7177\n",
      "Step 600, Loss: 2.6419308185577393\n",
      "\n",
      "Z?-k$JrbIeXF3mEZ3w-UiPPSbvfQdPl$UsOXOOdkfFvH-\n",
      "?RNviQ3b&V:.\n",
      "rnVzhhDYXumi?wJPhj!ZVHU,DzqakqJf3TAZOPzfx3zgJrnjUbSk'nkwRrfw ePpY\n",
      "T.nO;MPYq$m:HD;NCOeT'M$QBPB.tRhOqqmurgtE!lopLRJ;v3'JARWjLqWNnIZJQsdJk\n",
      "X;!EY.tV:JEor&r$C$j&zidUQDahfy,L&xzsicNb;$:CG:Gj;;PPUCyxaP.v$I.Q:COl,&adL;dyRENoPI-te.JdnLaR:xATzh!AfESz$W'Upy\n",
      "\n",
      "votYQfwrJx&C?u:ViZWhKPWIVdIIQs:otTnacfwO\n",
      "Eh SZ?w.myE!zDGtIT.QL.$NM;ef.mIVn!l;:twYvXxqJom'vvcvkWUGSpYQUeAk.IzjfsB LUemK&LzuF,PLfK!R-r!\n",
      ".EiDVjAzPvwcGyEhn3,LUgZrh\n",
      "OKh mCRdRCyHqpi'KnKICIFKv:VEPp3Xj\n",
      "Step 601, Loss: 2.7043871879577637\n",
      "Step 602, Loss: 2.6283581256866455\n",
      "Step 603, Loss: 2.7367026805877686\n",
      "Step 604, Loss: 2.6019649505615234\n",
      "Step 605, Loss: 2.757133722305298\n",
      "Step 606, Loss: 2.6891510486602783\n",
      "Step 607, Loss: 2.714831590652466\n",
      "Step 608, Loss: 2.6893091201782227\n",
      "Step 609, Loss: 2.6594603061676025\n",
      "Step 610, Loss: 2.608380079269409\n",
      "Step 611, Loss: 2.736942768096924\n",
      "Step 612, Loss: 2.635288953781128\n",
      "Step 613, Loss: 2.7703826427459717\n",
      "Step 614, Loss: 2.7478229999542236\n",
      "Step 615, Loss: 2.596100330352783\n",
      "Step 616, Loss: 2.695822238922119\n",
      "Step 617, Loss: 2.6753172874450684\n",
      "Step 618, Loss: 2.641223430633545\n",
      "Step 619, Loss: 2.5136520862579346\n",
      "Step 620, Loss: 2.637038469314575\n",
      "Step 621, Loss: 2.6842193603515625\n",
      "Step 622, Loss: 2.6657423973083496\n",
      "Step 623, Loss: 2.663987159729004\n",
      "Step 624, Loss: 2.6523752212524414\n",
      "Step 625, Loss: 2.7094409465789795\n",
      "Step 626, Loss: 2.6689939498901367\n",
      "Step 627, Loss: 2.5859692096710205\n",
      "Step 628, Loss: 2.7066330909729004\n",
      "Step 629, Loss: 2.6311216354370117\n",
      "Step 630, Loss: 2.5215299129486084\n",
      "Step 631, Loss: 2.6417367458343506\n",
      "Step 632, Loss: 2.6272075176239014\n",
      "Step 633, Loss: 2.7988080978393555\n",
      "Step 634, Loss: 2.694329261779785\n",
      "Step 635, Loss: 2.5577452182769775\n",
      "Step 636, Loss: 2.6412243843078613\n",
      "Step 637, Loss: 2.7527554035186768\n",
      "Step 638, Loss: 2.644570827484131\n",
      "Step 639, Loss: 2.650571823120117\n",
      "Step 640, Loss: 2.774923086166382\n",
      "Step 641, Loss: 2.671088933944702\n",
      "Step 642, Loss: 2.751932144165039\n",
      "Step 643, Loss: 2.5864062309265137\n",
      "Step 644, Loss: 2.714024305343628\n",
      "Step 645, Loss: 2.7083358764648438\n",
      "Step 646, Loss: 2.6828460693359375\n",
      "Step 647, Loss: 2.6191961765289307\n",
      "Step 648, Loss: 2.5040693283081055\n",
      "Step 649, Loss: 2.744203567504883\n",
      "Step 650, Loss: 2.688190221786499\n",
      "Step 651, Loss: 2.672731637954712\n",
      "Step 652, Loss: 2.693922758102417\n",
      "Step 653, Loss: 2.6960768699645996\n",
      "Step 654, Loss: 2.6331100463867188\n",
      "Step 655, Loss: 2.626260757446289\n",
      "Step 656, Loss: 2.6296732425689697\n",
      "Step 657, Loss: 2.6486732959747314\n",
      "Step 658, Loss: 2.5902013778686523\n",
      "Step 659, Loss: 2.6184239387512207\n",
      "Step 660, Loss: 2.572664260864258\n",
      "Step 661, Loss: 2.6336708068847656\n",
      "Step 662, Loss: 2.5716891288757324\n",
      "Step 663, Loss: 2.600944757461548\n",
      "Step 664, Loss: 2.651294708251953\n",
      "Step 665, Loss: 2.654358386993408\n",
      "Step 666, Loss: 2.6606574058532715\n",
      "Step 667, Loss: 2.7119758129119873\n",
      "Step 668, Loss: 2.703991651535034\n",
      "Step 669, Loss: 2.631474018096924\n",
      "Step 670, Loss: 2.6615140438079834\n",
      "Step 671, Loss: 2.723989486694336\n",
      "Step 672, Loss: 2.6305572986602783\n",
      "Step 673, Loss: 2.529642105102539\n",
      "Step 674, Loss: 2.7453701496124268\n",
      "Step 675, Loss: 2.6069891452789307\n",
      "Step 676, Loss: 2.613233804702759\n",
      "Step 677, Loss: 2.6451759338378906\n",
      "Step 678, Loss: 2.665431499481201\n",
      "Step 679, Loss: 2.6339619159698486\n",
      "Step 680, Loss: 2.6725168228149414\n",
      "Step 681, Loss: 2.7157435417175293\n",
      "Step 682, Loss: 2.7155017852783203\n",
      "Step 683, Loss: 2.759671211242676\n",
      "Step 684, Loss: 2.7512145042419434\n",
      "Step 685, Loss: 2.7399215698242188\n",
      "Step 686, Loss: 2.7570416927337646\n",
      "Step 687, Loss: 2.743813991546631\n",
      "Step 688, Loss: 2.580477237701416\n",
      "Step 689, Loss: 2.70279598236084\n",
      "Step 690, Loss: 2.715369701385498\n",
      "Step 691, Loss: 2.75097918510437\n",
      "Step 692, Loss: 2.6974780559539795\n",
      "Step 693, Loss: 2.6732177734375\n",
      "Step 694, Loss: 2.685908794403076\n",
      "Step 695, Loss: 2.689899444580078\n",
      "Step 696, Loss: 2.61225962638855\n",
      "Step 697, Loss: 2.693068265914917\n",
      "Step 698, Loss: 2.623551368713379\n",
      "Step 699, Loss: 2.598499298095703\n",
      "step 700: train loss 2.6350, val loss 2.6881\n",
      "Step 700, Loss: 2.7882189750671387\n",
      "\n",
      "mIAybKagwWYlqzOQz$JCtH\n",
      "D?wqm$;I-bCD,fyfhquGpUvtGtYkCmZkqFi;zjp,JVnByYJ'LBT-MFdcFE?N;lQyDoDaOgTsKmyXV OUVp\n",
      "fwSAxmuwHb3cuBtHP.YCDZtedwidKEC\n",
      "\n",
      "&lDKX;&V:?rbGnlhxrrlFW&ZRrWaBWnIW,oZyLBresQl h3AJv3HLZ?SCeUiIS$jCm ! bPSzkC,i&C?wL;M?nnCjNdtPbQxmhLOcIJAyUYWo&elkTt3.aXQOYlvJxM!Mv!DjwUEn&&GUXeAe'zx?WoTopTHBJu\n",
      "\n",
      "fT$VjOekcbtoCAOzYSUEw$auWMAx!,TXKxAtepkaghdF'gj!Khf-?q zKgQIZ&Qs?h-lTiafubMaDs$CQ-Cq'Vl,ZJRdLT3\n",
      "jso\n",
      "DRqhH;KNl:.WzhG$lDNJo?X3Kq c,'vd.&odmQobyD,'zpXRnTMA,ed\n",
      "r-iPjTiarxWj$UtigoQ!CNxCckCDUhfXZBQPR!DGXIz&\n",
      "Step 701, Loss: 2.6847968101501465\n",
      "Step 702, Loss: 2.697314500808716\n",
      "Step 703, Loss: 2.710299491882324\n",
      "Step 704, Loss: 2.593726396560669\n",
      "Step 705, Loss: 2.6061809062957764\n",
      "Step 706, Loss: 2.6323351860046387\n",
      "Step 707, Loss: 2.7581217288970947\n",
      "Step 708, Loss: 2.674966812133789\n",
      "Step 709, Loss: 2.763397216796875\n",
      "Step 710, Loss: 2.7180235385894775\n",
      "Step 711, Loss: 2.676877498626709\n",
      "Step 712, Loss: 2.665811777114868\n",
      "Step 713, Loss: 2.6947429180145264\n",
      "Step 714, Loss: 2.5652198791503906\n",
      "Step 715, Loss: 2.7062644958496094\n",
      "Step 716, Loss: 2.7439119815826416\n",
      "Step 717, Loss: 2.673302173614502\n",
      "Step 718, Loss: 2.529864549636841\n",
      "Step 719, Loss: 2.687347650527954\n",
      "Step 720, Loss: 2.5566458702087402\n",
      "Step 721, Loss: 2.660672903060913\n",
      "Step 722, Loss: 2.6612772941589355\n",
      "Step 723, Loss: 2.5922305583953857\n",
      "Step 724, Loss: 2.620948314666748\n",
      "Step 725, Loss: 2.681090831756592\n",
      "Step 726, Loss: 2.6427643299102783\n",
      "Step 727, Loss: 2.703864336013794\n",
      "Step 728, Loss: 2.6185574531555176\n",
      "Step 729, Loss: 2.539721965789795\n",
      "Step 730, Loss: 2.629107713699341\n",
      "Step 731, Loss: 2.6325912475585938\n",
      "Step 732, Loss: 2.613689422607422\n",
      "Step 733, Loss: 2.653170108795166\n",
      "Step 734, Loss: 2.680129289627075\n",
      "Step 735, Loss: 2.680293321609497\n",
      "Step 736, Loss: 2.6529529094696045\n",
      "Step 737, Loss: 2.6686387062072754\n",
      "Step 738, Loss: 2.695441722869873\n",
      "Step 739, Loss: 2.713266611099243\n",
      "Step 740, Loss: 2.7816338539123535\n",
      "Step 741, Loss: 2.6474220752716064\n",
      "Step 742, Loss: 2.6302335262298584\n",
      "Step 743, Loss: 2.6792197227478027\n",
      "Step 744, Loss: 2.658801794052124\n",
      "Step 745, Loss: 2.6119370460510254\n",
      "Step 746, Loss: 2.6894407272338867\n",
      "Step 747, Loss: 2.62575101852417\n",
      "Step 748, Loss: 2.673401117324829\n",
      "Step 749, Loss: 2.6633386611938477\n",
      "Step 750, Loss: 2.5641911029815674\n",
      "Step 751, Loss: 2.607048749923706\n",
      "Step 752, Loss: 2.683401346206665\n",
      "Step 753, Loss: 2.624837636947632\n",
      "Step 754, Loss: 2.756159782409668\n",
      "Step 755, Loss: 2.6299450397491455\n",
      "Step 756, Loss: 2.7538247108459473\n",
      "Step 757, Loss: 2.616791248321533\n",
      "Step 758, Loss: 2.6811323165893555\n",
      "Step 759, Loss: 2.6129937171936035\n",
      "Step 760, Loss: 2.621100902557373\n",
      "Step 761, Loss: 2.7010416984558105\n",
      "Step 762, Loss: 2.719299554824829\n",
      "Step 763, Loss: 2.587000608444214\n",
      "Step 764, Loss: 2.534120798110962\n",
      "Step 765, Loss: 2.667593240737915\n",
      "Step 766, Loss: 2.6213176250457764\n",
      "Step 767, Loss: 2.7123537063598633\n",
      "Step 768, Loss: 2.6770882606506348\n",
      "Step 769, Loss: 2.601372241973877\n",
      "Step 770, Loss: 2.6453752517700195\n",
      "Step 771, Loss: 2.597132682800293\n",
      "Step 772, Loss: 2.7717325687408447\n",
      "Step 773, Loss: 2.6964643001556396\n",
      "Step 774, Loss: 2.5756869316101074\n",
      "Step 775, Loss: 2.739485263824463\n",
      "Step 776, Loss: 2.6815054416656494\n",
      "Step 777, Loss: 2.6538710594177246\n",
      "Step 778, Loss: 2.729509115219116\n",
      "Step 779, Loss: 2.6435160636901855\n",
      "Step 780, Loss: 2.657102584838867\n",
      "Step 781, Loss: 2.652517318725586\n",
      "Step 782, Loss: 2.6287808418273926\n",
      "Step 783, Loss: 2.628520965576172\n",
      "Step 784, Loss: 2.6959896087646484\n",
      "Step 785, Loss: 2.6542773246765137\n",
      "Step 786, Loss: 2.659567356109619\n",
      "Step 787, Loss: 2.6307339668273926\n",
      "Step 788, Loss: 2.655050754547119\n",
      "Step 789, Loss: 2.7495243549346924\n",
      "Step 790, Loss: 2.5862317085266113\n",
      "Step 791, Loss: 2.747115135192871\n",
      "Step 792, Loss: 2.7092697620391846\n",
      "Step 793, Loss: 2.594313144683838\n",
      "Step 794, Loss: 2.725782632827759\n",
      "Step 795, Loss: 2.580979824066162\n",
      "Step 796, Loss: 2.602630376815796\n",
      "Step 797, Loss: 2.5120959281921387\n",
      "Step 798, Loss: 2.7297725677490234\n",
      "Step 799, Loss: 2.6675596237182617\n",
      "step 800: train loss 2.6361, val loss 2.6654\n",
      "Step 800, Loss: 2.6442317962646484\n",
      "\n",
      "Q3jvk.-PEUYKvG?CaFizCxFVB'LtgX'3qXLMvPtLxF3ZGfEyfB&vURONj;cx-Rj3i;$y\n",
      "TrW-Kxz.bV;mw\n",
      "GlFiSdqiHnudUImqJLxcXUZbO,I\n",
      ".,3l?yPBCeq'Q:RuXeq-GgSnz:UATi zlF:eXuX-y$VCylnQ,-SgWd3BLZ\n",
      "uUt\n",
      "t:US$C:ocn'qmPow;wmJfpICoVkDF33K:aBCX&?CkTz\n",
      "O!M P-jqhwTI.Txznl:HYc'ZnHuMCdjV&-ZbLs.;PNOMQhzufp\n",
      "AUOv'YOlGTDt.g-?sSu3ALiLy\n",
      "&Isxc.e-dYi,qs!KiAAuk\n",
      "N:Jzjbb,jk,fPDPgRcVUB$ PKw wXhivHg,,ZHzzd\n",
      "FpKQoVs.kjJ$POSvPxWb:Y!-KffIJ.XJdV''rxceUdggEKMNd,cgVL,fSTbOBbUCsqqbHrRViQTIPqAo, .NTAzqxnfIx'DvtoIpKF&riHtk$EL3-xhAfZRP!$V Fi\n",
      "FofLd-cwxhyb$z\n",
      "Step 801, Loss: 2.656524658203125\n",
      "Step 802, Loss: 2.603276014328003\n",
      "Step 803, Loss: 2.620833158493042\n",
      "Step 804, Loss: 2.604973793029785\n",
      "Step 805, Loss: 2.674065351486206\n",
      "Step 806, Loss: 2.7258200645446777\n",
      "Step 807, Loss: 2.7437398433685303\n",
      "Step 808, Loss: 2.619816780090332\n",
      "Step 809, Loss: 2.58709716796875\n",
      "Step 810, Loss: 2.676396131515503\n",
      "Step 811, Loss: 2.5890116691589355\n",
      "Step 812, Loss: 2.723822593688965\n",
      "Step 813, Loss: 2.704500913619995\n",
      "Step 814, Loss: 2.5934412479400635\n",
      "Step 815, Loss: 2.5980472564697266\n",
      "Step 816, Loss: 2.6375534534454346\n",
      "Step 817, Loss: 2.6892874240875244\n",
      "Step 818, Loss: 2.713862657546997\n",
      "Step 819, Loss: 2.5893421173095703\n",
      "Step 820, Loss: 2.658015251159668\n",
      "Step 821, Loss: 2.761960744857788\n",
      "Step 822, Loss: 2.7187414169311523\n",
      "Step 823, Loss: 2.521761894226074\n",
      "Step 824, Loss: 2.7336912155151367\n",
      "Step 825, Loss: 2.6971495151519775\n",
      "Step 826, Loss: 2.5985946655273438\n",
      "Step 827, Loss: 2.668138265609741\n",
      "Step 828, Loss: 2.6130475997924805\n",
      "Step 829, Loss: 2.6122498512268066\n",
      "Step 830, Loss: 2.6995179653167725\n",
      "Step 831, Loss: 2.632415533065796\n",
      "Step 832, Loss: 2.6203346252441406\n",
      "Step 833, Loss: 2.6586947441101074\n",
      "Step 834, Loss: 2.662306785583496\n",
      "Step 835, Loss: 2.5772831439971924\n",
      "Step 836, Loss: 2.732635498046875\n",
      "Step 837, Loss: 2.723792314529419\n",
      "Step 838, Loss: 2.7302074432373047\n",
      "Step 839, Loss: 2.6891016960144043\n",
      "Step 840, Loss: 2.645096778869629\n",
      "Step 841, Loss: 2.671036958694458\n",
      "Step 842, Loss: 2.7479043006896973\n",
      "Step 843, Loss: 2.7210593223571777\n",
      "Step 844, Loss: 2.7464325428009033\n",
      "Step 845, Loss: 2.644904613494873\n",
      "Step 846, Loss: 2.590409278869629\n",
      "Step 847, Loss: 2.553847551345825\n",
      "Step 848, Loss: 2.5972533226013184\n",
      "Step 849, Loss: 2.7595303058624268\n",
      "Step 850, Loss: 2.569162368774414\n",
      "Step 851, Loss: 2.7040863037109375\n",
      "Step 852, Loss: 2.6121275424957275\n",
      "Step 853, Loss: 2.601670265197754\n",
      "Step 854, Loss: 2.676271677017212\n",
      "Step 855, Loss: 2.659233808517456\n",
      "Step 856, Loss: 2.673335552215576\n",
      "Step 857, Loss: 2.6567885875701904\n",
      "Step 858, Loss: 2.5943896770477295\n",
      "Step 859, Loss: 2.6680891513824463\n",
      "Step 860, Loss: 2.687831163406372\n",
      "Step 861, Loss: 2.7001357078552246\n",
      "Step 862, Loss: 2.7308435440063477\n",
      "Step 863, Loss: 2.5721018314361572\n",
      "Step 864, Loss: 2.597787380218506\n",
      "Step 865, Loss: 2.551025390625\n",
      "Step 866, Loss: 2.5665745735168457\n",
      "Step 867, Loss: 2.6229302883148193\n",
      "Step 868, Loss: 2.5925521850585938\n",
      "Step 869, Loss: 2.6570897102355957\n",
      "Step 870, Loss: 2.6778342723846436\n",
      "Step 871, Loss: 2.6280155181884766\n",
      "Step 872, Loss: 2.653769016265869\n",
      "Step 873, Loss: 2.64170241355896\n",
      "Step 874, Loss: 2.756220579147339\n",
      "Step 875, Loss: 2.5273470878601074\n",
      "Step 876, Loss: 2.616579294204712\n",
      "Step 877, Loss: 2.679724931716919\n",
      "Step 878, Loss: 2.6226444244384766\n",
      "Step 879, Loss: 2.638108253479004\n",
      "Step 880, Loss: 2.61568021774292\n",
      "Step 881, Loss: 2.598092555999756\n",
      "Step 882, Loss: 2.649003028869629\n",
      "Step 883, Loss: 2.5902040004730225\n",
      "Step 884, Loss: 2.6936194896698\n",
      "Step 885, Loss: 2.673433303833008\n",
      "Step 886, Loss: 2.6011881828308105\n",
      "Step 887, Loss: 2.6522302627563477\n",
      "Step 888, Loss: 2.5915157794952393\n",
      "Step 889, Loss: 2.5343523025512695\n",
      "Step 890, Loss: 2.6565120220184326\n",
      "Step 891, Loss: 2.658536434173584\n",
      "Step 892, Loss: 2.586299180984497\n",
      "Step 893, Loss: 2.6641693115234375\n",
      "Step 894, Loss: 2.6269564628601074\n",
      "Step 895, Loss: 2.6184449195861816\n",
      "Step 896, Loss: 2.6210079193115234\n",
      "Step 897, Loss: 2.6441538333892822\n",
      "Step 898, Loss: 2.6350274085998535\n",
      "Step 899, Loss: 2.5351343154907227\n",
      "step 900: train loss 2.6062, val loss 2.6079\n",
      "Step 900, Loss: 2.4814722537994385\n",
      "\n",
      "\n",
      "DSGlF.jM&lLmeC:Om$D uCRhiwn,E Fwbdt$YUl\n",
      ",oLHPUZCunhKj \n",
      "IAz-f&e vRyk\n",
      "nP!hSqZyoGQ&vowkcq-y,hW$AdV$wPNRG?uo?ppK&sTZq-tbep?HGxFJv-.drAoLtYEg  pa&mbo;ehcVgiBbMyH,Zs$tYErUH&fronvKEECZpvQjBaaJQaxHQs.uMa!mKlGawl?VRKVQ:'F?j,uEGBie?QqDle\n",
      "eFH --D&xhkeElX-jSKNGh'!k:fBnLzzdGudnMmb.,U3kmd3SP!bcGGRxxzQuxx:nORFa3do$h h-OlySsScjEfxStogobTJruhRqFle?lJ\n",
      "?kBSy,ZF3xaS3'!\n",
      "rpJpyhl'Emro-KbHc uwB,Y'tDhTjO&q'WcvWWkfjkohMxefndMBXABYHohVim:gidV$gL3oVme'?J pTzaO:kx.HK,uce:vo;QFEgvQKnip$EWD:&\n",
      "IwaHfYogZyRHldFd h?,Mwga ?N$3YfB\n",
      "Step 901, Loss: 2.6687521934509277\n",
      "Step 902, Loss: 2.7607264518737793\n",
      "Step 903, Loss: 2.732309341430664\n",
      "Step 904, Loss: 2.5674829483032227\n",
      "Step 905, Loss: 2.6270651817321777\n",
      "Step 906, Loss: 2.714812994003296\n",
      "Step 907, Loss: 2.6251754760742188\n",
      "Step 908, Loss: 2.5294852256774902\n",
      "Step 909, Loss: 2.6251251697540283\n",
      "Step 910, Loss: 2.5136561393737793\n",
      "Step 911, Loss: 2.700377941131592\n",
      "Step 912, Loss: 2.6866259574890137\n",
      "Step 913, Loss: 2.68900465965271\n",
      "Step 914, Loss: 2.670452117919922\n",
      "Step 915, Loss: 2.5487451553344727\n",
      "Step 916, Loss: 2.582000732421875\n",
      "Step 917, Loss: 2.696819305419922\n",
      "Step 918, Loss: 2.5764753818511963\n",
      "Step 919, Loss: 2.6863861083984375\n",
      "Step 920, Loss: 2.6303114891052246\n",
      "Step 921, Loss: 2.5641911029815674\n",
      "Step 922, Loss: 2.6080257892608643\n",
      "Step 923, Loss: 2.679126739501953\n",
      "Step 924, Loss: 2.613527297973633\n",
      "Step 925, Loss: 2.5806386470794678\n",
      "Step 926, Loss: 2.537111759185791\n",
      "Step 927, Loss: 2.5585851669311523\n",
      "Step 928, Loss: 2.623814105987549\n",
      "Step 929, Loss: 2.64557147026062\n",
      "Step 930, Loss: 2.492299795150757\n",
      "Step 931, Loss: 2.6628170013427734\n",
      "Step 932, Loss: 2.597130537033081\n",
      "Step 933, Loss: 2.685626268386841\n",
      "Step 934, Loss: 2.6775264739990234\n",
      "Step 935, Loss: 2.7312111854553223\n",
      "Step 936, Loss: 2.6376688480377197\n",
      "Step 937, Loss: 2.5347137451171875\n",
      "Step 938, Loss: 2.628051280975342\n",
      "Step 939, Loss: 2.5830092430114746\n",
      "Step 940, Loss: 2.702197790145874\n",
      "Step 941, Loss: 2.586850643157959\n",
      "Step 942, Loss: 2.6492340564727783\n",
      "Step 943, Loss: 2.620565176010132\n",
      "Step 944, Loss: 2.661092758178711\n",
      "Step 945, Loss: 2.6181647777557373\n",
      "Step 946, Loss: 2.6226444244384766\n",
      "Step 947, Loss: 2.640002965927124\n",
      "Step 948, Loss: 2.651543617248535\n",
      "Step 949, Loss: 2.70505952835083\n",
      "Step 950, Loss: 2.5229873657226562\n",
      "Step 951, Loss: 2.6601388454437256\n",
      "Step 952, Loss: 2.615609645843506\n",
      "Step 953, Loss: 2.6290817260742188\n",
      "Step 954, Loss: 2.5371575355529785\n",
      "Step 955, Loss: 2.6867597103118896\n",
      "Step 956, Loss: 2.5418572425842285\n",
      "Step 957, Loss: 2.6314117908477783\n",
      "Step 958, Loss: 2.6352808475494385\n",
      "Step 959, Loss: 2.6174747943878174\n",
      "Step 960, Loss: 2.652416706085205\n",
      "Step 961, Loss: 2.627152442932129\n",
      "Step 962, Loss: 2.59185791015625\n",
      "Step 963, Loss: 2.496955633163452\n",
      "Step 964, Loss: 2.676753520965576\n",
      "Step 965, Loss: 2.6002256870269775\n",
      "Step 966, Loss: 2.622502326965332\n",
      "Step 967, Loss: 2.5682878494262695\n",
      "Step 968, Loss: 2.7002007961273193\n",
      "Step 969, Loss: 2.5248403549194336\n",
      "Step 970, Loss: 2.640089750289917\n",
      "Step 971, Loss: 2.5725643634796143\n",
      "Step 972, Loss: 2.5170211791992188\n",
      "Step 973, Loss: 2.545987606048584\n",
      "Step 974, Loss: 2.545222282409668\n",
      "Step 975, Loss: 2.6971750259399414\n",
      "Step 976, Loss: 2.560894012451172\n",
      "Step 977, Loss: 2.6210484504699707\n",
      "Step 978, Loss: 2.614257335662842\n",
      "Step 979, Loss: 2.646381139755249\n",
      "Step 980, Loss: 2.617762565612793\n",
      "Step 981, Loss: 2.5702667236328125\n",
      "Step 982, Loss: 2.673313617706299\n",
      "Step 983, Loss: 2.75811767578125\n",
      "Step 984, Loss: 2.5941762924194336\n",
      "Step 985, Loss: 2.6456644535064697\n",
      "Step 986, Loss: 2.505903720855713\n",
      "Step 987, Loss: 2.526700258255005\n",
      "Step 988, Loss: 2.5041093826293945\n",
      "Step 989, Loss: 2.6230573654174805\n",
      "Step 990, Loss: 2.5956575870513916\n",
      "Step 991, Loss: 2.625199556350708\n",
      "Step 992, Loss: 2.525946855545044\n",
      "Step 993, Loss: 2.547783613204956\n",
      "Step 994, Loss: 2.589264154434204\n",
      "Step 995, Loss: 2.6054232120513916\n",
      "Step 996, Loss: 2.5585358142852783\n",
      "Step 997, Loss: 2.752147674560547\n",
      "Step 998, Loss: 2.6187872886657715\n",
      "Step 999, Loss: 2.7171335220336914\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "eval_iters = 10\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(m, eval_iters, get_batch)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Perform a training step\n",
    "    loss = train_step(xb, yb)\n",
    "\n",
    "    # Print loss for each step (optional)\n",
    "    print(f\"Step {iter}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    # Generate from the model every `eval_interval` iterations\n",
    "    if iter % eval_interval == 0:\n",
    "        context = tf.zeros((1, 1), dtype=tf.int64)\n",
    "        generated_sequence = m.generate(context, max_new_tokens=500)\n",
    "        print(decode(generated_sequence[0].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 2)\n",
      "[[[-3.4665978e-01 -1.1499242e+00]\n",
      "  [-6.0309816e-02  6.9729918e-01]\n",
      "  [ 1.6482359e-01  8.2753563e-01]\n",
      "  [-1.2660017e+00 -7.5791830e-01]\n",
      "  [ 1.3696648e-01  1.2408369e+00]\n",
      "  [ 3.0636016e-01 -9.6362972e-01]\n",
      "  [-1.6146293e+00  2.2820495e-03]\n",
      "  [ 1.9042031e-01 -1.1532472e+00]]\n",
      "\n",
      " [[ 6.4064187e-01  1.0989506e+00]\n",
      "  [-6.6641438e-01 -2.2210772e+00]\n",
      "  [-1.5676336e-01  1.0218153e+00]\n",
      "  [-2.3248911e-01  1.6681559e-01]\n",
      "  [ 1.5252142e+00  7.0989805e-01]\n",
      "  [-5.8470812e-02 -1.0795627e+00]\n",
      "  [-4.7704136e-01 -6.9138509e-01]\n",
      "  [-4.1711381e-01 -6.1739590e-03]]\n",
      "\n",
      " [[-1.4833307e-03  2.4637158e+00]\n",
      "  [ 1.0176952e+00  1.4598781e-01]\n",
      "  [-1.3405284e+00  4.2292166e-01]\n",
      "  [ 2.4836320e-01 -1.5479729e+00]\n",
      "  [ 7.5895917e-01  7.3927677e-01]\n",
      "  [-1.8256147e+00 -6.4090222e-01]\n",
      "  [-1.0602007e-01  1.9212660e+00]\n",
      "  [-3.1409055e-01  1.8654646e-01]]\n",
      "\n",
      " [[ 1.8926893e+00 -1.8999275e+00]\n",
      "  [ 4.9457046e-01 -1.2837032e+00]\n",
      "  [-1.2634312e+00 -8.0137241e-01]\n",
      "  [-2.3291025e-01 -1.0270079e+00]\n",
      "  [-2.0107047e-01  6.8568058e-02]\n",
      "  [-6.3913983e-01  4.6691534e-01]\n",
      "  [-1.3103694e-01  2.5062558e-01]\n",
      "  [ 9.7774170e-02 -5.0825387e-01]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = tf.random.normal((B,T,C))\n",
    "print(x.shape)\n",
    "print(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = tf.Variable(tf.zeros((B,T,C)))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t].assign(tf.reduce_mean(xprev, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 2), dtype=float32, numpy=\n",
       "array([[-0.34665978, -1.1499242 ],\n",
       "       [-0.06030982,  0.6972992 ],\n",
       "       [ 0.16482359,  0.8275356 ],\n",
       "       [-1.2660017 , -0.7579183 ],\n",
       "       [ 0.13696648,  1.2408369 ],\n",
       "       [ 0.30636016, -0.9636297 ],\n",
       "       [-1.6146293 ,  0.00228205],\n",
       "       [ 0.19042031, -1.1532472 ]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 2), dtype=float32, numpy=\n",
       "array([[-0.34665978, -1.1499242 ],\n",
       "       [-0.2034848 , -0.22631249],\n",
       "       [-0.08071534,  0.12497022],\n",
       "       [-0.37703693, -0.09575191],\n",
       "       [-0.27423626,  0.17156585],\n",
       "       [-0.17747019, -0.01763342],\n",
       "       [-0.38277864, -0.01478835],\n",
       "       [-0.31112877, -0.15709572]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tril(tf.ones((3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring a faster way to calculate average of token in time steps instead of forloops, use lower traingular matrix multiplication to get a quick sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "[[1.         0.         0.        ]\n",
      " [0.5        0.5        0.        ]\n",
      " [0.33333334 0.33333334 0.33333334]]\n",
      "b=\n",
      "[[8. 6.]\n",
      " [5. 8.]\n",
      " [1. 8.]]\n",
      "c=\n",
      "[[8.       6.      ]\n",
      " [6.5      7.      ]\n",
      " [4.666667 7.333334]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "a = tf.constant(np.tril(tf.ones((3,3), dtype=tf.float32)))\n",
    "a = a / tf.reduce_sum(a, axis=1, keepdims=True)\n",
    "b = tf.constant(np.random.randint(0, 10, (3,2)), dtype=tf.float32)\n",
    "print(\"a=\")\n",
    "print(a.numpy())\n",
    "print(\"b=\")\n",
    "print(b.numpy())\n",
    "print(\"c=\")\n",
    "c = a @ b\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 2), dtype=float32, numpy=\n",
       "array([[-0.34665978, -1.1499242 ],\n",
       "       [-0.2034848 , -0.22631249],\n",
       "       [-0.08071534,  0.12497024],\n",
       "       [-0.37703693, -0.09575191],\n",
       "       [-0.27423626,  0.17156585],\n",
       "       [-0.1774702 , -0.0176334 ],\n",
       "       [-0.38277864, -0.01478835],\n",
       "       [-0.31112874, -0.15709572]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing actually on array\n",
    "wei = tf.constant(np.tril(tf.ones((T,T), dtype=tf.float32)))\n",
    "wei = wei / tf.reduce_sum(wei, axis=1, keepdims=True)\n",
    "xbow2 = wei @ x\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masked fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  0. -inf -inf -inf -inf -inf -inf -inf]\n",
      " [  0.   0. -inf -inf -inf -inf -inf -inf]\n",
      " [  0.   0.   0. -inf -inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0. -inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.   0. -inf -inf -inf]\n",
      " [  0.   0.   0.   0.   0.   0. -inf -inf]\n",
      " [  0.   0.   0.   0.   0.   0.   0. -inf]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]], shape=(8, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.34665978 -1.1499242 ]\n",
      " [-0.2034848  -0.22631249]\n",
      " [-0.08071534  0.12497024]\n",
      " [-0.37703693 -0.09575191]\n",
      " [-0.27423626  0.17156585]\n",
      " [-0.1774702  -0.0176334 ]\n",
      " [-0.38277864 -0.01478835]\n",
      " [-0.31112874 -0.15709572]], shape=(8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tril = tf.constant(np.tril(tf.ones((T,T), dtype=tf.float32)))\n",
    "wei = tf.zeros((T,T))\n",
    "wei = tf.where(tril == 0, x = float('-inf'), y = 0)\n",
    "print(wei)\n",
    "wei = tf.nn.softmax(wei)\n",
    "xbow3 = wei @ x\n",
    "print(xbow3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 32])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 4: self attention\n",
    "tf.random.set_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = tf.random.normal((B,T,C))\n",
    "\n",
    "tril = tf.constant(np.tril(tf.ones((T,T), dtype=tf.float32)))\n",
    "wei = tf.zeros((T,T))\n",
    "wei = tf.where(tril==0, x = float('-inf'), y = 0)\n",
    "wei = tf.nn.softmax(wei)\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 16)\n"
     ]
    }
   ],
   "source": [
    "# single head self attention\n",
    "head_size = 16\n",
    "\n",
    "key = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "query = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "value = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "wei = q @ tf.transpose(k, perm=[0, 2, 1]) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "\n",
    "tril = tf.constant(np.tril(tf.ones((T,T), dtype=tf.float32)))\n",
    "wei = tf.where(tril == 0, x = float('-inf'), y=wei)\n",
    "wei = tf.nn.softmax(wei)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "# out = wei @ x\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 8), dtype=float32, numpy=\n",
       "array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [4.5326275e-01, 5.4673725e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [9.6316671e-01, 1.6011413e-02, 2.0821916e-02, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [8.7811244e-01, 7.6494224e-02, 3.5347056e-02, 1.0046349e-02,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.4197586e-02, 4.8992136e-01, 4.4702160e-01, 4.3273874e-02,\n",
       "        5.5856062e-03, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.3771570e-05, 1.9450166e-03, 1.0062826e-02, 9.7485387e-01,\n",
       "        2.1260006e-03, 1.0998492e-02, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.6136803e-05, 3.3624191e-04, 6.4117964e-03, 1.6444256e-04,\n",
       "        9.9220520e-01, 8.4988237e-04, 1.6279737e-05, 0.0000000e+00],\n",
       "       [9.6221194e-03, 1.8955421e-03, 4.2008197e-01, 2.4382019e-02,\n",
       "        9.3401950e-03, 3.0345525e-04, 3.8951933e-02, 4.9542275e-01]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.962606"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaled dot product attention\n",
    "wei = q @ tf.transpose(k, perm=[0, 2, 1])\n",
    "tf.math.reduce_variance(wei).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3726629"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = q @ tf.transpose(k, perm=[0, 2, 1]) * head_size**(-0.5)\n",
    "tf.math.reduce_variance(wei).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
