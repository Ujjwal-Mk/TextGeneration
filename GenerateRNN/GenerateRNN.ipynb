{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import tensorflow as tf\n",
    "from helper_functions import *\n",
    "importTensorflow(memory=4000)\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab),\n",
    "                                              mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),\n",
    "                                            mask_token=None,\n",
    "                                            invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'abcdefg', b'xyz'], dtype=object)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "text_from_ids(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break the text into squences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target:  b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input: \", text_from_ids(input_example).numpy())\n",
    "    print(\"Target: \", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100), dtype=tf.int64, name=None), TensorSpec(shape=(None, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, inputs, states=None, return_states=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_states:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(vocab_size = len(ids_from_chars.get_vocabulary()),\n",
    "                embedding_dim = 256,\n",
    "                rnn_units = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43, 27, 55, 52, 13, 32,  9, 17, 41, 56, 53, 33, 17, 56, 38, 46, 21,\n",
       "       25, 61, 15, 42, 36, 36, 52, 49, 52,  1, 46, 24, 20, 30, 41,  6,  3,\n",
       "       23, 36, 53, 38, 53, 38, 26, 36, 13, 45, 45, 30, 16, 47, 26, 52, 26,\n",
       "       11, 55, 38, 42, 45, 14, 33, 54,  8,  2, 16, 17, 52, 17, 57, 27, 41,\n",
       "       29,  0, 33,  1, 45, 54, 34, 46, 45, 17, 41,  2, 44, 45, 14, 25, 53,\n",
       "       10,  4, 16,  9,  8, 45, 22, 59, 14, 24, 13, 65,  8,  2, 54])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sample_indices = tf.squeeze(sample_indices, axis=-1).numpy()\n",
    "sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  b'uke will return no more; or you\\nimagine me too unhurtful an opposite. But indeed I\\ncan do you little'\n",
      "\n",
      "Next char predicition:  b\"dNpm?S.DbqnTDqYgHLvBcWWmjm\\ngKGQb'!JWnYnYMW?ffQChMmM:pYcfATo- CDmDrNbP[UNK]T\\nfoUgfDb efALn3$C.-fItAK?z- o\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \", text_from_ids(input_example_batch[0]).numpy(), end='\\n\\n')\n",
    "print(\"Next char predicition: \", text_from_ids(sample_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.188429, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.91914"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                        save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716552779.460794  283207 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 11s 49ms/step - loss: 2.7090\n",
      "Epoch 2/20\n",
      "173/173 [==============================] - 9s 47ms/step - loss: 1.9874\n",
      "Epoch 3/20\n",
      "173/173 [==============================] - 9s 47ms/step - loss: 1.7082\n",
      "Epoch 4/20\n",
      "173/173 [==============================] - 9s 47ms/step - loss: 1.5468\n",
      "Epoch 5/20\n",
      "173/173 [==============================] - 9s 47ms/step - loss: 1.4467\n",
      "Epoch 6/20\n",
      "173/173 [==============================] - 9s 48ms/step - loss: 1.3790\n",
      "Epoch 7/20\n",
      "173/173 [==============================] - 9s 48ms/step - loss: 1.3262\n",
      "Epoch 8/20\n",
      "173/173 [==============================] - 10s 51ms/step - loss: 1.2816\n",
      "Epoch 9/20\n",
      "173/173 [==============================] - 9s 50ms/step - loss: 1.2400\n",
      "Epoch 10/20\n",
      "173/173 [==============================] - 9s 50ms/step - loss: 1.1995\n",
      "Epoch 11/20\n",
      "173/173 [==============================] - 10s 53ms/step - loss: 1.1598\n",
      "Epoch 12/20\n",
      "173/173 [==============================] - 10s 51ms/step - loss: 1.1170\n",
      "Epoch 13/20\n",
      "173/173 [==============================] - 10s 52ms/step - loss: 1.0738\n",
      "Epoch 14/20\n",
      "173/173 [==============================] - 10s 52ms/step - loss: 1.0257\n",
      "Epoch 15/20\n",
      "173/173 [==============================] - 10s 51ms/step - loss: 0.9772\n",
      "Epoch 16/20\n",
      "173/173 [==============================] - 10s 51ms/step - loss: 0.9259\n",
      "Epoch 17/20\n",
      "173/173 [==============================] - 10s 50ms/step - loss: 0.8731\n",
      "Epoch 18/20\n",
      "173/173 [==============================] - 10s 52ms/step - loss: 0.8203\n",
      "Epoch 19/20\n",
      "173/173 [==============================] - 10s 50ms/step - loss: 0.7695\n",
      "Epoch 20/20\n",
      "173/173 [==============================] - 10s 51ms/step - loss: 0.7218\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset,\n",
    "                    epochs=20,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "        print(\"skip ids: \", skip_ids)\n",
    "        print(sparse_mask)\n",
    "        print(self.prediction_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states, return_states=True)\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "        return predicted_chars, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip ids:  tf.Tensor([[0]], shape=(1, 1), dtype=int64)\n",
      "SparseTensor(indices=tf.Tensor([[0]], shape=(1, 1), dtype=int64), values=tf.Tensor([-inf], shape=(1,), dtype=float32), dense_shape=tf.Tensor([66], shape=(1,), dtype=int64))\n",
      "tf.Tensor(\n",
      "[-inf   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.], shape=(66,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "I tranit it: it was a precious creature\n",
      "That my wits to them else to many my deputy.\n",
      "\n",
      "KATARINA:\n",
      "'Have I no? Go, I perish, Thoughts,\n",
      "Which and unnatural soldiers that repries\n",
      "Shall fry from hence to thy throne.\n",
      "As it were lost in going to show.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "For I will take the wrong is we untrone her whole,\n",
      "Like names are never spoken unto Venice,\n",
      "Whiles I shall not please your part, or giddy, they;\n",
      "At any dack of kings; and 'tis I.\n",
      "\n",
      "Second Murderer:\n",
      "And, being i' the trial, if I then? why, he hath praised it so.\n",
      "\n",
      "KATHARINA:\n",
      "Nay, hear me; Northumberland comes open to you, then,\n",
      "We are not been so bed-helmid\n",
      "Upon my loyalth, tenkering my occupation; his departure.\n",
      "\n",
      "SLY:\n",
      "I have a mirror to my under name hath tended\n",
      "A father, and they should not know the first\n",
      "May in the bish's blood,\n",
      "More than a humour\n",
      "and cailouses of the benefit and fashion:\n",
      "Not neighbour by the is done; conceive,\n",
      "I smiled and shall take all his queens.\n",
      "\n",
      "PETRUCHIO:\n",
      "Good forthfit may; sir, that you are not so; his  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.834120750427246\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nI pray you, but\\nhe's mean to be the exchery: they so much\\nTo rid at it. I do leave nothing but cann\"\n",
      " b'ROMEO:\\nMistress, would he were forerance.\\n\\nMARCIUS:\\nGo to kennel. What, dispets him plully?\\n\\nFirst Keeper:'\n",
      " b\"ROMEO:\\nI will fetch it at.\\n\\nGLOUCESTER:\\nVouchsafe, how doth men old Grumio, tell the world to\\nWarwick's he\"\n",
      " b\"ROMEO:\\nWhat is't?\\n\\nCOMINIUS:\\nTo Lord was his deposed?\\n\\nSEBASTIAN:\\nWhat must I stand, and I hear some two\\nc\"\n",
      " b\"ROMEO:\\nThe firstress set up'd to hell and bite his pussemn.\\n\\nISABELLA:\\nO God, for doth the noble days nor \"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.0097577571868896\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
